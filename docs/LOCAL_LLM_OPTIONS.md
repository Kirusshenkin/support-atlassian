# –õ–æ–∫–∞–ª—å–Ω—ã–µ LLM –¥–ª—è AI Confluence Assistant

## üè† –ó–∞—á–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏?

1. **–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å** - –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç –≤–∞—à—É –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
2. **–≠–∫–æ–Ω–æ–º–∏—è** - –Ω–µ—Ç –ø–ª–∞—Ç—ã –∑–∞ API –ø–æ—Å–ª–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
3. **–ö–æ–Ω—Ç—Ä–æ–ª—å** - –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –º–æ–¥–µ–ª—å—é –∏ –µ—ë –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
4. **–°–∫–æ—Ä–æ—Å—Ç—å** - –Ω–µ—Ç –∑–∞–¥–µ—Ä–∂–µ–∫ –Ω–∞ —Å–µ—Ç–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
5. **–ö–∞—á–µ—Å—Ç–≤–æ** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç YandexGPT

## üÜö –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ vs YandexGPT

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ | YandexGPT |
|----------|------------------|-----------|
| –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ | ‚≠ê‚≠ê‚≠ê‚≠ê (Llama 3.1, Qwen 2.5) | ‚≠ê‚≠ê |
| –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Qwen 2.5, Saiga) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| –°—Ç–æ–∏–º–æ—Å—Ç—å | –¢–æ–ª—å–∫–æ –∂–µ–ª–µ–∑–æ | –ü–ª–∞—Ç–Ω–æ –∑–∞ —Ç–æ–∫–µ–Ω—ã |
| –°–∫–æ—Ä–æ—Å—Ç—å (—Å GPU) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| –ü—Ä–æ—Å—Ç–æ—Ç–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**–í—ã–≤–æ–¥**: –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å GPU —Å–µ—Ä–≤–µ—Ä –∏–ª–∏ –º–æ—â–Ω—ã–π CPU, –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–æ—Å–æ–±–µ–Ω–Ω–æ Qwen 2.5 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ) –¥–∞–¥—É—Ç –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —á–µ–º YandexGPT.

## ü§ñ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏

### 1. **Llama 3.1** ‚≠ê –õ—É—á—à–∏–π –≤—ã–±–æ—Ä
```python
# –†–∞–∑–º–µ—Ä—ã: 8B, 70B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
# –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: 
# - 8B: –º–∏–Ω–∏–º—É–º 16GB RAM –∏–ª–∏ 8GB VRAM
# - 70B: –º–∏–Ω–∏–º—É–º 64GB RAM –∏–ª–∏ 48GB VRAM

# –í requirements.txt
llama-cpp-python>=0.2.0

# –í .env
LLM_PROVIDER=llamacpp
MODEL_PATH=./models/llama-3.1-8b-instruct.Q4_K_M.gguf
```

### 2. **Mistral 7B**
```python
# –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
# –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: 8-16GB RAM

# –í .env
LLM_PROVIDER=llamacpp
MODEL_PATH=./models/mistral-7b-instruct-v0.3.Q4_K_M.gguf
```

### 3. **Qwen 2.5** üåü –õ—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
```python
# –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º
# –ö–∞—á–µ—Å—Ç–≤–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å GPT-3.5, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç YandexGPT
# –†–∞–∑–º–µ—Ä—ã: 7B, 14B, 32B, 72B

# –í .env
LLM_PROVIDER=llamacpp
MODEL_PATH=./models/qwen2.5-14b-instruct.Q4_K_M.gguf  # —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 14B
```

**–ü–æ—á–µ–º—É Qwen –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ:**
- –û–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
- –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –Ω—é–∞–Ω—Å—ã —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
- –†–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ, —á–µ–º YandexGPT Pro

### 4. **Saiga (Llama-based)** üá∑üá∫
```python
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
# –ë–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ Llama 3

# –í .env
LLM_PROVIDER=llamacpp
MODEL_PATH=./models/saiga-llama3-8b.Q4_K_M.gguf
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ llama-cpp-python:

```bash
# CPU –≤–µ—Ä—Å–∏—è
pip install llama-cpp-python

# GPU –≤–µ—Ä—Å–∏—è (NVIDIA)
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python

# GPU –≤–µ—Ä—Å–∏—è (Apple Metal)
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

### 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:

```bash
# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
mkdir -p models

# –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å (–ø—Ä–∏–º–µ—Ä –¥–ª—è Llama 3.1)
wget https://huggingface.co/TheBloke/Llama-3.1-8B-Instruct-GGUF/resolve/main/llama-3.1-8b-instruct.Q4_K_M.gguf -O models/llama-3.1-8b-instruct.Q4_K_M.gguf
```

### 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ qa_service.py:

```python
def create_llm():
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è LLM"""
    provider = os.getenv("LLM_PROVIDER", "openai")
    
    if provider == "llamacpp":
        from langchain_community.llms import LlamaCpp
        
        model_path = os.getenv("MODEL_PATH", "./models/llama-3.1-8b-instruct.Q4_K_M.gguf")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GPU
        n_gpu_layers = int(os.getenv("N_GPU_LAYERS", "-1"))  # -1 = –≤—Å–µ —Å–ª–æ–∏ –Ω–∞ GPU
        
        return LlamaCpp(
            model_path=model_path,
            temperature=0.3,
            max_tokens=2048,
            n_ctx=4096,  # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            n_gpu_layers=n_gpu_layers,
            n_batch=512,
            f16_kv=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å float16 –¥–ª—è –∫—ç—à–∞
            verbose=False
        )
    
    elif provider == "ollama":
        from langchain_community.llms import Ollama
        
        return Ollama(
            base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            model=os.getenv("OLLAMA_MODEL", "llama3.1:8b"),
            temperature=0.3
        )
    
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
```

## üê≥ –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Ollama (–ø—Ä–æ—â–µ)

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama:
```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# –ò–ª–∏ —á–µ—Ä–µ–∑ Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

### 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏:
```bash
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
ollama pull llama3.1:8b

# –ò–ª–∏ Mistral
ollama pull mistral:7b-instruct

# –ò–ª–∏ Qwen
ollama pull qwen2.5:7b
```

### 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤ .env:
```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
```

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | RAM/VRAM | –¢–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ (CPU) | –¢–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ (GPU) |
|--------|--------|----------|-------------------|-------------------|
| Llama 3.1 8B Q4 | 4.7GB | 8GB | 5-10 | 30-50 |
| Mistral 7B Q4 | 4.1GB | 6GB | 8-15 | 40-60 |
| Qwen 2.5 7B Q4 | 4.5GB | 8GB | 6-12 | 35-55 |
| Llama 3.1 70B Q4 | 40GB | 48GB | 0.5-2 | 10-20 |

## üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### 1. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è:
- **Q4_K_M** - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
- **Q5_K_M** - –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–µ–¥–ª–µ–Ω–Ω–µ–µ
- **Q3_K_S** - –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ

### 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU:
```env
# –í—Å–µ —Å–ª–æ–∏ –Ω–∞ GPU (–µ—Å–ª–∏ –ø–æ–º–µ—â–∞–µ—Ç—Å—è)
N_GPU_LAYERS=-1

# –ß–∞—Å—Ç–∏—á–Ω–æ –Ω–∞ GPU (–µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç VRAM)
N_GPU_LAYERS=20
```

### 3. –ë–∞—Ç—á–∏–Ω–≥:
```python
# –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è GPU
n_batch=1024  # –≤–º–µ—Å—Ç–æ 512
```

## ‚öñÔ∏è –û–±–ª–∞–∫–æ vs –õ–æ–∫–∞–ª—å–Ω–æ

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±–ª–∞–∫–æ –µ—Å–ª–∏:
- ‚úÖ –ù—É–∂–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (GPT-4, Claude)
- ‚úÖ –ù–µ—Ç –º–æ—â–Ω–æ–≥–æ –∂–µ–ª–µ–∑–∞
- ‚úÖ –ù—É–∂–Ω–∞ –ø—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
- ‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω–æ –µ—Å–ª–∏:
- ‚úÖ –ö—Ä–∏—Ç–∏—á–Ω–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å
- ‚úÖ –ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º –∑–∞–ø—Ä–æ—Å–æ–≤
- ‚úÖ –ï—Å—Ç—å GPU —Å–µ—Ä–≤–µ—Ä
- ‚úÖ –ù—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –î–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤:
1. **–ü–µ—Ä–≤—ã–π –≤—ã–±–æ—Ä**: Qwen 2.5 14B –∏–ª–∏ 32B (–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç YandexGPT)
2. **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞**: Saiga-Llama3 8B (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
3. **–ï—Å–ª–∏ –º–∞–ª–æ –ø–∞–º—è—Ç–∏**: Qwen 2.5 7B –∏–ª–∏ Mistral 7B

### –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
1. **–ù–∞—á–Ω–∏—Ç–µ —Å –æ–±–ª–∞–∫–∞** –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞ (Anthropic Claude)
2. **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏** –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
3. **–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥**: 
   - –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
   - –û–±–ª–∞—á–Ω–∞—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤

```python
def create_hybrid_llm():
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –ª–æ–∫–∞–ª—å–Ω–∞—è + –æ–±–ª–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å"""
    
    # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    local_llm = create_local_llm()
    
    # –û–±–ª–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    cloud_llm = create_cloud_llm()
    
    # –õ–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    def select_llm(question: str) -> Any:
        # –ü—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –ª–æ–∫–∞–ª—å–Ω–æ
        if len(question) < 50 and "?" in question:
            return local_llm
        # –°–ª–æ–∂–Ω—ã–µ - –≤ –æ–±–ª–∞–∫–æ
        return cloud_llm
    
    return select_llm 
```
